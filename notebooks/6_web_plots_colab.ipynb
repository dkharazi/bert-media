{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Python libraries\n",
    "!pip install -r https://raw.githubusercontent.com/dkharazi/bert-news/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for generic data preprocessing\n",
    "import os\n",
    "import json\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "# Import libraries for preprocessing embeddings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import libraries for classifying clusters based on embeddings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "\n",
    "# Import libraries for clustering and topic classification\n",
    "import hdbscan\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet('https://github.com/dkharazi/bert-news/blob/main/data/proc_articles.gzip?raw=true')\n",
    "articles = articles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Stemming and Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in stop words, stemmer, and regex tokenizer\n",
    "stop = stopwords.words('english')\n",
    "punc = RegexpTokenizer(r'\\w+')\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# Define function for de-lemmatizing words, removing whitespace, and stop words\n",
    "def reformat_articles(w):\n",
    "    # lowecase and remove stop words\n",
    "    words = [word.lower() for word in w.split() if word.lower() not in stop]\n",
    "    # remove punctuation\n",
    "    words = [punc.tokenize(word) for word in words]\n",
    "    # remove whitespace\n",
    "    words = list(chain.from_iterable(words))\n",
    "    # remove stems\n",
    "    # words = [stemmer.stem(word) for word in words]  # TODO -- SEE IF IMPROVED AFTER DELETING THIS LINE\n",
    "    # convert from list to string\n",
    "    new_article = ' '.join(words)\n",
    "    return new_article\n",
    "\n",
    "# Apply lemmatization, whitespace removal, and stop word removal\n",
    "articles['content'] = articles['content'].apply(reformat_articles)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pre-trained DistilBERT model\n",
    "# DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode embeddings\n",
    "embeddings = model.encode(articles['content'], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mean-centering standardization on embeddings\n",
    "std_embeddings = StandardScaler(with_mean=True).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood-Based Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non-linear dimensionality reduction\n",
    "# The smaller the value, the more localized the dimensionality reduction\n",
    "# The larger the value, the more globalized the dimensionality reduction\n",
    "# n_neighbors = 15\n",
    "# n_components = 5\n",
    "# min_dist = 0.01\n",
    "# metric = cosine\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, n_components=5, min_dist=0.01, metric='cosine', random_state=42).fit_transform(std_embeddings)\n",
    "\n",
    "# Perform min-max standardization on embeddings\n",
    "std_umap_embeddings = MinMaxScaler().fit_transform(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform density-based clustering on dimensionality reduced embeddings\n",
    "# The smaller the value, the more clusters returned\n",
    "# The larger the value, the fewer clusters returned\n",
    "# min_cluster_size=100\n",
    "# metric=euclidean\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=100, metric='euclidean').fit(std_umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of each cluster/label\n",
    "unique, counts = np.unique(cluster.labels_, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.01, metric='cosine', random_state=27).fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=10)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=10, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mapping of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping from cluster to topic\n",
    "clust_map = {\n",
    "    0: 'Baseball',\n",
    "    1: 'Nuclear',\n",
    "    2: 'Cuba Relations',\n",
    "    3: 'Space',\n",
    "    4: 'Terrorism and War',\n",
    "    5: 'Crime',\n",
    "    6: 'Brexit',\n",
    "    7: 'Football',\n",
    "    8: 'Olympics',\n",
    "    9: 'Basketball',\n",
    "    10: 'Protesting and Activism',\n",
    "    11: 'LGBTQ Discrimination',\n",
    "    12: 'Segregation and Racial Discrimination',\n",
    "    13: 'Politics',\n",
    "    14: 'U.S. Illegal Immigration',\n",
    "    15: 'Nazism and Syrian Refugee Crisis',\n",
    "    16: 'Global Warming',\n",
    "    17: 'Technology and Data Privacy',\n",
    "    18: 'Food',\n",
    "    19: 'Medical Research',\n",
    "    20: 'Widespread Disease',\n",
    "    21: 'Real Estate',\n",
    "    22: 'Business and Finance',\n",
    "    23: 'Music',\n",
    "    24: 'Pop Culture and Entertainment'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Topics JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- Remove row filter from line 5\n",
    "# Add cluster and embeddings to dataframe\n",
    "articles['cluster'] = cluster.labels_\n",
    "articles = articles.replace(clust_map)\n",
    "articles['umap_embed1'] = umap_data[:,0]\n",
    "articles['umap_embed2'] = umap_data[:,1]\n",
    "topics = articles[articles['cluster'] != -1].reset_index(drop=True)\n",
    "topics = topics.reset_index()\n",
    "\n",
    "# Prepare dataframe for JSON formatting\n",
    "topics = topics.rename(columns={'index': 'article_id'})\n",
    "topics = topics[['article_id', 'publication', 'party', 'cluster', 'umap_embed1', 'umap_embed2']]\n",
    "\n",
    "# Convert dataframe to JSON\n",
    "topics_str = topics.to_json(orient=\"records\")\n",
    "topics_json = json.loads(topics_str)\n",
    "topics_str = json.dumps(topics_json, indent=4)\n",
    "\n",
    "# Take a glimpse at JSON!\n",
    "print(topics_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top Words for each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign articles to topics\n",
    "docs_df = pd.DataFrame(articles['content'])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df = docs_df[docs_df['Topic'] != -1]\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "\n",
    "# Rename columns\n",
    "docs_df.columns = ['Doc', 'Topic', 'Doc_ID']\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "docs_per_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute c-TF-IDF scores for each word\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(articles['content']))\n",
    "tf_idf[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top words for each topic\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = [{'topic': clust_map[label], 'top20_words': [{'word': words[j], 'tfidf': tf_idf_transposed[i][j]} for j in indices[i]][::-1]} for i, label in enumerate(labels)]\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF-IDF JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to JSON\n",
    "tfidf_str = json.dumps(top_n_words, indent=4)\n",
    "\n",
    "# Take a glimpse at JSON!\n",
    "print(tfidf_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Topics based on Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for XGBoost training\n",
    "idx = cluster.labels_ != -1\n",
    "y = cluster.labels_[idx]\n",
    "X = embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get same number of randomly chosen test indices\n",
    "idx = []\n",
    "min_lab = min(Counter(y_test).values())\n",
    "for i in np.unique(y_test):\n",
    "    idx.extend(np.random.choice(np.where(y_test==i)[0], min_lab, replace=False))\n",
    "\n",
    "# Enforce similar frequency of labels in test data\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try undersampling to 1000 articles for each cluster\n",
    "large_c = [c for c, count in Counter(y_train.tolist()).items() if count >= 1000]\n",
    "small_c = [c for c, count in Counter(y_train.tolist()).items() if count < 1000]\n",
    "idx = np.where(np.isin(y_train, small_c))[0].tolist()\n",
    "for i in large_c:\n",
    "    idx.extend(np.random.choice(np.where(y_train==i)[0], 1000, replace=False))\n",
    "\n",
    "# Enforce similar frequency of labels in training data\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTEN object for oversampling nominal features\n",
    "sampler = SMOTEN(random_state=12)\n",
    "\n",
    "# Simulate oversampled data for imbalanced classes\n",
    "X_res, y_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check previous work!\n",
    "print(Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build xgboost model to predict topics based on embeddings\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    tree_method='gpu_hist',\n",
    "    predictor='gpu_predictor',\n",
    "    use_label_encoder=False\n",
    "    )\n",
    "\n",
    "# Train model\n",
    "model = xgb.fit(X_res, y_res, eval_metric=['mlogloss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model accuracy\n",
    "preds = model.predict(X_res)\n",
    "round((preds == y_res).sum().astype(float) / len(preds)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of embedding names\n",
    "feature_names = np.array(['Embedding {}'.format(str(i)) for i in range(1,len(model.feature_importances_)+1)])\n",
    "\n",
    "# Get top-20 most important features\n",
    "sorted_idx = model.feature_importances_.argsort()[0:20]\n",
    "\n",
    "# Plot feature importances\n",
    "plt.barh(feature_names[sorted_idx], model.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"XGBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Top BERT Embeddings to Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign top-3 embeddings to each article\n",
    "articles['bert_embed1'] = embeddings[:,694]\n",
    "articles['bert_embed2'] = embeddings[:,193]\n",
    "articles['bert_embed3'] = embeddings[:,162]\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any articles without definitive clusters\n",
    "embed_df = articles[articles['cluster'] != -1].reset_index(drop=True)\n",
    "\n",
    "# Visualize top-3 embeddings with 3D graph\n",
    "# and color each point based on their topic\n",
    "fig = px.scatter_3d(embed_df, x='bert_embed1', y='bert_embed2', z='bert_embed3', color='cluster')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Final File as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat strings as dictionaries\n",
    "topics_json = json.loads(topics_str)\n",
    "tfidf_json = json.loads(tfidf_str)\n",
    "\n",
    "# Combine dictionaries together\n",
    "merged_dict = {'topics_data': topics_json, 'tfidf_data': tfidf_json}\n",
    "\n",
    "# Dump dictionary to JSON file\n",
    "with open('/data/news.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Plotting XGBoost and Shap Values](https://github.com/slundberg/shap#tree-ensemble-example-xgboostlightgbmcatboostscikit-learnpyspark-models)\n",
    "- [Text plots with Shap values](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html#text-plot)\n",
    "- [Partitioning data with Shap values](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/explainers/Exact.html#Tabular-data-with-partition-(Owen-value)-masking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
