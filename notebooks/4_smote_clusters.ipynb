{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for generic data preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "# Import libraries for model selection and accuracy measures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Import BERT transformer libraries\n",
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet('../data/cluster_articles.gzip')\n",
    "articles = articles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publication</th>\n",
       "      <th>content</th>\n",
       "      <th>party</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breitbart Launches ’Border Wall Construction C...</td>\n",
       "      <td>Milo</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>last weekend church confessed sin personal van...</td>\n",
       "      <td>right</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IDF Airstrike Eliminates 4 Islamic State-Linke...</td>\n",
       "      <td>Breitbart Jerusalem</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>times israel reports israeli airstrike killed ...</td>\n",
       "      <td>right</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oracle Funds Anti-Google Effort that Outs Hill...</td>\n",
       "      <td>Chriss W. Street</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>oracle corporation using deep financial resour...</td>\n",
       "      <td>right</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Silicon Valley Urges Giving Election Day Off t...</td>\n",
       "      <td>Chriss W. Street</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>apparently worried populist movement led donal...</td>\n",
       "      <td>right</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Illegal Migrant Abandoned in Desert Calls 911 ...</td>\n",
       "      <td>Bob Price</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>severely dehydrated illegal alien called 911 p...</td>\n",
       "      <td>right</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title               author  \\\n",
       "0  Breitbart Launches ’Border Wall Construction C...                 Milo   \n",
       "1  IDF Airstrike Eliminates 4 Islamic State-Linke...  Breitbart Jerusalem   \n",
       "2  Oracle Funds Anti-Google Effort that Outs Hill...     Chriss W. Street   \n",
       "3  Silicon Valley Urges Giving Election Day Off t...     Chriss W. Street   \n",
       "4  Illegal Migrant Abandoned in Desert Calls 911 ...            Bob Price   \n",
       "\n",
       "  publication                                            content  party  \\\n",
       "0   Breitbart  last weekend church confessed sin personal van...  right   \n",
       "1   Breitbart  times israel reports israeli airstrike killed ...  right   \n",
       "2   Breitbart  oracle corporation using deep financial resour...  right   \n",
       "3   Breitbart  apparently worried populist movement led donal...  right   \n",
       "4   Breitbart  severely dehydrated illegal alien called 911 p...  right   \n",
       "\n",
       "   cluster  \n",
       "0       -1  \n",
       "1        4  \n",
       "2       13  \n",
       "3       -1  \n",
       "4        5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any articles without definitive clusters\n",
    "articles = articles[articles['cluster'] != -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BERT Tokenizer and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Initialize BERT configurations\n",
    "dist_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=25)\n",
    "\n",
    "# Implement pre-trained BERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=dist_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize articles\n",
    "tokenized_articles = tokenizer(\n",
    "    text=articles['content'].tolist(),\n",
    "    return_tensors='pt',\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokenized input IDs\n",
    "pt_articles = tokenized_articles['input_ids']\n",
    "\n",
    "# Reformat cluster column as tensor\n",
    "pt_clusters = tensor(articles['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths:  24498 4324 24498 4324\n",
      "Counts:  Counter({13: 12238, 5: 4002, 4: 1969, 16: 1399, 24: 1127, 18: 1079, 1: 786, 22: 732, 23: 650, 10: 571, 7: 530, 0: 457, 3: 371, 11: 350, 17: 347, 21: 339, 9: 269, 8: 261, 15: 252, 6: 249, 14: 239, 19: 188, 20: 184, 2: 120, 12: 113})\n"
     ]
    }
   ],
   "source": [
    "# Split data and stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pt_articles,\n",
    "    pt_clusters,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=pt_clusters\n",
    "    )\n",
    "\n",
    "# Print lengths\n",
    "print('Lengths: ', len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "# Classes are imbalanced, so we must oversample training data and undersample test data\n",
    "print('Counts: ', Counter(pt_clusters.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample Test Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat test tensor as numpy array\n",
    "yt = y_test.cpu().detach().numpy()\n",
    "\n",
    "# Get same number of randomly chosen test indices\n",
    "idx = []\n",
    "min_lab = min(Counter(yt).values())\n",
    "for i in np.unique(yt):\n",
    "    idx.extend(np.random.choice(np.where(yt==i)[0], min_lab, replace=False))\n",
    "\n",
    "# Enforce similar frequency of labels in test data\n",
    "X_test = X_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of each label in entire dataset:\n",
      "\n",
      "Counter({13: 12238, 5: 4002, 4: 1969, 16: 1399, 24: 1127, 18: 1079, 1: 786, 22: 732, 23: 650, 10: 571, 7: 530, 0: 457, 3: 371, 11: 350, 17: 347, 21: 339, 9: 269, 8: 261, 15: 252, 6: 249, 14: 239, 19: 188, 20: 184, 2: 120, 12: 113})\n",
      "---\n",
      "Frequency of each label in training set:\n",
      "\n",
      "Counter({13: 10402, 5: 3402, 4: 1674, 16: 1189, 24: 958, 18: 917, 1: 668, 22: 622, 23: 552, 10: 485, 7: 451, 0: 388, 3: 315, 11: 298, 17: 295, 21: 288, 9: 229, 8: 222, 15: 214, 6: 212, 14: 203, 19: 160, 20: 156, 2: 102, 12: 96})\n",
      "---\n",
      "Frequency of each label in test set:\n",
      "\n",
      "Counter({0: 17, 1: 17, 2: 17, 3: 17, 4: 17, 5: 17, 6: 17, 7: 17, 8: 17, 9: 17, 10: 17, 11: 17, 12: 17, 13: 17, 14: 17, 15: 17, 16: 17, 17: 17, 18: 17, 19: 17, 20: 17, 21: 17, 22: 17, 23: 17, 24: 17})\n"
     ]
    }
   ],
   "source": [
    "# Print distribution of each label\n",
    "print('Frequency of each label in entire dataset:')\n",
    "print()\n",
    "print(Counter(pt_clusters.tolist()))\n",
    "print('---')\n",
    "print('Frequency of each label in training set:')\n",
    "print()\n",
    "print(Counter(y_train.tolist()))\n",
    "print('---')\n",
    "print('Frequency of each label in test set:')\n",
    "print()\n",
    "print(Counter(y_test.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Oversampling/Undersampling of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({13: 10402, 5: 3402, 4: 1674, 16: 1189, 24: 958, 18: 917, 1: 668, 22: 622, 23: 552, 10: 485, 7: 451, 0: 388, 3: 315, 11: 298, 17: 295, 21: 288, 9: 229, 8: 222, 15: 214, 6: 212, 14: 203, 19: 160, 20: 156, 2: 102, 12: 96})\n"
     ]
    }
   ],
   "source": [
    "# First, undersample the outlandish number of poltical articles\n",
    "print(Counter(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({13: 1000, 5: 1000, 4: 1000, 16: 1000, 24: 958, 18: 917, 1: 668, 22: 622, 23: 552, 10: 485, 7: 451, 0: 388, 3: 315, 11: 298, 17: 295, 21: 288, 9: 229, 8: 222, 15: 214, 6: 212, 14: 203, 19: 160, 20: 156, 2: 102, 12: 96})\n"
     ]
    }
   ],
   "source": [
    "# Reformat training tensor as numpy array\n",
    "yt = y_train.cpu().detach().numpy()\n",
    "\n",
    "# Try undersampling to 1000 articles for each cluster\n",
    "large_c = [c for c, count in Counter(y_train.tolist()).items() if count >= 1000]\n",
    "small_c = [c for c, count in Counter(y_train.tolist()).items() if count < 1000]\n",
    "idx = np.where(np.isin(yt, small_c))[0].tolist()\n",
    "for i in large_c:\n",
    "    idx.extend(np.random.choice(np.where(yt==i)[0], 1000, replace=False))\n",
    "\n",
    "# Enforce similar frequency of labels in training data\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "# Print undersampled counts of each label\n",
    "print(Counter(y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTEN object for oversampling nominal features\n",
    "sampler = SMOTEN(random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate oversampled data for imbalanced classes\n",
    "X_res, y_res = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat numpy arrays as pytorch tensors\n",
    "X_res, y_res = tensor(X_res).long(), tensor(y_res).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({11: 1000, 17: 1000, 22: 1000, 6: 1000, 18: 1000, 10: 1000, 1: 1000, 24: 1000, 15: 1000, 21: 1000, 3: 1000, 20: 1000, 7: 1000, 23: 1000, 19: 1000, 8: 1000, 14: 1000, 0: 1000, 12: 1000, 9: 1000, 2: 1000, 13: 1000, 5: 1000, 4: 1000, 16: 1000})\n"
     ]
    }
   ],
   "source": [
    "# Print oversampled counts of each label\n",
    "print(Counter(y_res.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset with articles\n",
    "class ArticlesDataset(Dataset):\n",
    "    def __init__(self, embeddings, clusters):\n",
    "        self.encodings = embeddings\n",
    "        self.labels = clusters\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.encodings[idx], 'labels': self.labels[idx]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Reformat training data as PyTorch Dataset\n",
    "train_dataset = ArticlesDataset(X_res, y_res)\n",
    "\n",
    "# Reformat test data as PyTorch Dataset\n",
    "test_dataset = ArticlesDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BERT Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../bert_results',    # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='../bert_logs',      # logging directory\n",
    ")\n",
    "\n",
    "# Implement Trainer object for training on articles and clusters\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22c1d48e9690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train BERT\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SMOTEN data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction: ' + str(np.argmax(trainer.predict(test_dataset).predictions)))\n",
    "print('actual: ' + str(trainer.predict(test_dataset).label_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search and output model producing best accuracies using SMOTEN\n",
    "# Then, compare with model oversampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
